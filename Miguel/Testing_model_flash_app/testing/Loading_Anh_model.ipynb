{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Resources/optimized_predict_pricing.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Resources/optimized_predict_pricing.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the dataset for reference (you need to provide the path to your dataset)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df_encoded1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResources/df_encoded1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Update the path if necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Miguel\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Resources/optimized_predict_pricing.pkl'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('./Resources/optimized_predict_pricing.pkl')\n",
    "\n",
    "# Load the dataset for reference (you need to provide the path to your dataset)\n",
    "df_encoded1 = pd.read_csv('Resources/df_encoded1.csv')  # Update the path if necessary\n",
    "\n",
    "# Check the columns of df_encoded1\n",
    "print(\"Columns in df_encoded1:\", df_encoded1.columns)\n",
    "\n",
    "# Function to handle default values for user inputs\n",
    "def get_user_input(prompt, default_value, value_type):\n",
    "    user_input = input(f\"{prompt} (default {default_value}): \")\n",
    "    if not user_input:\n",
    "        return value_type(default_value)\n",
    "    try:\n",
    "        return value_type(user_input)\n",
    "    except ValueError:\n",
    "        print(f\"Invalid input. Using default value: {default_value}\")\n",
    "        return value_type(default_value)\n",
    "\n",
    "# Preprocessing function (to ensure consistency with model training)\n",
    "def preprocess_input_data(input_data):\n",
    "    # One-hot encode the 'city' and 'zipcode' columns, similar to training time\n",
    "    input_data_encoded = pd.get_dummies(input_data, columns=['city', 'zipcode'], drop_first=True)\n",
    "    \n",
    "    # Get all the columns from the original dataset used for training, including one-hot encoding\n",
    "    model_columns = list(df_encoded1.columns)\n",
    "    \n",
    "    # Ensure all columns are present (add missing columns with 0)\n",
    "    missing_cols = set(model_columns) - set(input_data_encoded.columns)\n",
    "    for col in missing_cols:\n",
    "        input_data_encoded[col] = 0  # Add missing columns with 0\n",
    "    \n",
    "    # Ensure the column order is the same as during training\n",
    "    input_data_encoded = input_data_encoded[model_columns]\n",
    "    \n",
    "    return input_data_encoded\n",
    "\n",
    "# Main function for suggestion\n",
    "def suggest_property():\n",
    "    print(\"Please enter the following details (leave empty to use default values):\")\n",
    "    \n",
    "    # User inputs with defaults, using helper function\n",
    "    bedrooms = get_user_input(\"Number of Bedrooms\", 3, int)\n",
    "    bathrooms = get_user_input(\"Number of Bathrooms\", 2.0, float)\n",
    "    sqft_living = get_user_input(\"Square Feet of Living Area\", 1500, int)\n",
    "    avg_income = get_user_input(\"Average Income\", 65000, float)\n",
    "    city = get_user_input(\"City\", \"Seattle\", str)\n",
    "    zipcode = get_user_input(\"Zipcode\", \"98105\", str)\n",
    "    min_price = int(input(\"Minimum Price: \"))\n",
    "    max_price = int(input(\"Maximum Price: \"))\n",
    "\n",
    "    # Prepare the input data for prediction\n",
    "    input_data = pd.DataFrame([[bedrooms, bathrooms, sqft_living, city, zipcode, avg_income]],\n",
    "                              columns=['bedrooms', 'bathrooms', 'sqft_living', 'city', 'zipcode', 'avg_income'])\n",
    "\n",
    "    # Preprocess the input data (encode categorical variables)\n",
    "    input_data_encoded = preprocess_input_data(input_data)\n",
    "\n",
    "    # Make the prediction using the trained model\n",
    "    predicted_price = model1.predict(input_data_encoded)[0]\n",
    "    print(f\"The predicted price for your request is: ${predicted_price:,.2f}\")\n",
    "\n",
    "    # Filter dataset for price range\n",
    "    filtered_df = df_encoded1[(df_encoded1['price'] >= min_price) & (df_encoded1['price'] <= max_price)]\n",
    "\n",
    "    # Find the closest predicted price in the filtered dataset\n",
    "    closest = filtered_df.iloc[(filtered_df['price'] - predicted_price).abs().argmin()]\n",
    "\n",
    "    # Suggest the best city and zipcode\n",
    "    print(f\"Suggested City: {closest['city']}\")\n",
    "    print(f\"Suggested Zipcode: {closest['zipcode']}\")\n",
    "    print(f\"Price in Suggested City: ${closest['price']:,.2f}\")\n",
    "\n",
    "# Run the suggestion function\n",
    "suggest_property()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "    # Load the model from the file\n",
    "model = joblib.load('../static_data/optimized_predict_pricing.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['bedrooms' 'bathrooms' 'sqft_living' 'avg_income' 'city_Auburn'\n",
      " 'city_Bellevue' 'city_Federal Way' 'city_Kent' 'city_Kirkland'\n",
      " 'city_Redmond' 'city_Renton' 'city_Sammamish' 'city_Seattle'\n",
      " 'zipcode_98001' 'zipcode_98002' 'zipcode_98003' 'zipcode_98004'\n",
      " 'zipcode_98005' 'zipcode_98006' 'zipcode_98007' 'zipcode_98008'\n",
      " 'zipcode_98023' 'zipcode_98030' 'zipcode_98031' 'zipcode_98032'\n",
      " 'zipcode_98033' 'zipcode_98034' 'zipcode_98042' 'zipcode_98052'\n",
      " 'zipcode_98053' 'zipcode_98055' 'zipcode_98056' 'zipcode_98058'\n",
      " 'zipcode_98059' 'zipcode_98074' 'zipcode_98075' 'zipcode_98092'\n",
      " 'zipcode_98102' 'zipcode_98103' 'zipcode_98105' 'zipcode_98106'\n",
      " 'zipcode_98107' 'zipcode_98108' 'zipcode_98109' 'zipcode_98112'\n",
      " 'zipcode_98115' 'zipcode_98116' 'zipcode_98117' 'zipcode_98118'\n",
      " 'zipcode_98119' 'zipcode_98122' 'zipcode_98125' 'zipcode_98126'\n",
      " 'zipcode_98133' 'zipcode_98136' 'zipcode_98144' 'zipcode_98146'\n",
      " 'zipcode_98148' 'zipcode_98155' 'zipcode_98166' 'zipcode_98168'\n",
      " 'zipcode_98177' 'zipcode_98178' 'zipcode_98188' 'zipcode_98198'\n",
      " 'zipcode_98199']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\", model.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'target_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mtarget_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'target_name'"
     ]
    }
   ],
   "source": [
    "print(\"Target name:\", model.target_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
